{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14006db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ef35e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5101aa87",
   "metadata": {},
   "source": [
    "# Importing Climate Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e14a790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"climatebert/distilroberta-base-climate-f\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"climatebert/distilroberta-base-climate-f\",output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20654c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f140ba",
   "metadata": {},
   "source": [
    "# Removing tokens with size < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b37c4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(filter(lambda x: len(x)>4, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c8b14267",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for ids,sen in enumerate(vocab):\n",
    "\n",
    "#     marked_text = \"[CLS] \" + sen + \" [SEP]\"\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = []\n",
    "    tokenized_text.append(\"[CLS] \")\n",
    "    tokenized_text.append(sen)\n",
    "    tokenized_text.append(\" [SEP]\")\n",
    "\n",
    "    \n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    #creating segment ids for the sentence\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "\n",
    "        hidden_states = outputs[-1]\n",
    "\n",
    "\n",
    "    #print('Tensor shape for each layer: ', hidden_states[0].size())\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # token_embeddings.size()\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat.append(sum_vec)\n",
    "\n",
    "    target.append(torch.stack(token_vecs_cat, dim = 0).mean(dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fe645881",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.stack(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "26a3cd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35873, 768)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6b014b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = a\n",
    "normed_embeddings = (all_embeddings.T / (all_embeddings**2).sum(axis=1) ** 0.5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "60a2096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#making kd tree\n",
    "indexer = KDTree(normed_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "00c73f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['Electric vehicle',\n",
    "'Solar',\n",
    "'Wind',\n",
    "'Hydroelectric' ,\n",
    "'Nuclear',\n",
    "'REC',\n",
    "'Efficiency',\n",
    "'Deforestation',\n",
    "'Afforestation',\n",
    "'carbon',\n",
    "'credit',\n",
    "'capture',\n",
    "'sequestration',\n",
    "'storage',\n",
    "'Hydrogen',\n",
    "'Geothermal',\n",
    "'Biomass',\n",
    "'Renewable', \n",
    "'Energy',\n",
    "'emissions',\n",
    "'reforestation',\n",
    "'Decreased', \n",
    "'Reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d587c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for ids,sen in enumerate(keywords):\n",
    "    \n",
    "    marked_text = \"[CLS] \" + sen + \" [SEP]\"\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "\n",
    "    #creating segment ids for the sentence\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "\n",
    "        hidden_states = outputs[-1]\n",
    "\n",
    "\n",
    "    #print('Tensor shape for each layer: ', hidden_states[0].size())\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # token_embeddings.size()\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat.append(sum_vec)\n",
    "        \n",
    "    target.append(torch.stack(token_vecs_cat, dim = 0).mean(dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9741af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for emb in zip(target,keywords):\n",
    "    top_20 = indexer.query(emb[0].reshape(1, -1),return_distance = False, k = 20)\n",
    "#     print(top_20)\n",
    "    result.append([emb[1],np.array(vocab)[top_20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7daff94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Electric vehicle',\n",
       "  array([['Electric', 'Battery', 'Solar', 'Tesla', 'energy', 'Motor',\n",
       "          'Power', 'ĠMobility', 'Energy', 'Steel', 'Volume', 'Water',\n",
       "          'Animal', 'electric', 'Environmental', 'Europe', 'Street',\n",
       "          'electricity', 'Australia', 'Ġpavement']], dtype='<U128')],\n",
       " ['Solar',\n",
       "  array([['Solar', 'Australia', 'Steel', 'Battery', 'Tesla', 'Power',\n",
       "          'Europe', 'energy', 'ĠPower', 'Canada', 'Japan', 'Spain',\n",
       "          'Energy', 'Water', 'California', 'India', 'Volume',\n",
       "          'electricity', 'Electric', 'ĠMountain']], dtype='<U128')],\n",
       " ['Wind',\n",
       "  array([['Solar', 'Power', 'Australia', 'Europe', 'ĠPower', 'ĠWind',\n",
       "          'Atlantic', 'Spain', 'Battery', 'Steel', 'Canada', 'Water',\n",
       "          'ĠMountain', 'Network', 'Volume', 'Electric', 'energy', 'Energy',\n",
       "          'Japan', 'STATE']], dtype='<U128')],\n",
       " ['Hydroelectric',\n",
       "  array([['Solar', 'Electric', 'Power', 'Water', 'energy', 'Energy',\n",
       "          'Australia', 'electricity', 'Steel', 'electric', 'Metro',\n",
       "          'Europe', 'River', 'Environmental', 'ĠPower', 'Volume',\n",
       "          'Network', 'Battery', 'industrial', 'Canada']], dtype='<U128')],\n",
       " ['Nuclear',\n",
       "  array([['Solar', 'Steel', 'Australia', 'Battery', 'China', 'Network',\n",
       "          'Canada', 'Energy', 'Power', 'Europe', 'Water', 'energy',\n",
       "          'ĠPower', 'Primary', 'Japan', 'Environmental', 'marine',\n",
       "          'ĠMountain', 'Labour', 'Canadian']], dtype='<U128')],\n",
       " ['REC',\n",
       "  array([['Volume', 'electricity', 'ĠPower', 'Solar', 'Water', 'ĠWater',\n",
       "          'Australia', 'Ġrebate', 'energy', 'product', 'Ġpavement',\n",
       "          'Primary', 'Power', 'Battery', 'chemical', 'income', 'ĠMobility',\n",
       "          'Ġwater', 'District', 'water']], dtype='<U128')],\n",
       " ['Efficiency',\n",
       "  array([['Volume', 'Energy', 'Water', 'Temperature', 'Environmental',\n",
       "          'Strength', 'energy', 'Policy', 'Cooldown', 'Power', 'Battery',\n",
       "          'income', 'ĠWater', 'Parameter', 'Solar', 'ĠPower',\n",
       "          'ĠPercentage', 'Primary', 'Technical', 'Physical']], dtype='<U128')],\n",
       " ['Deforestation',\n",
       "  array([['Climate', 'forestation', 'Environmental', 'Water', 'Australia',\n",
       "          'Forest', 'forest', 'Solar', 'Temperature', 'ĠForest', 'Closure',\n",
       "          'Volume', 'energy', 'Ġdrainage', 'Energy', 'Policy',\n",
       "          'industrial', 'climate', 'Program', 'Legal']], dtype='<U128')],\n",
       " ['Afforestation',\n",
       "  array([['Solar', 'energy', 'plant', 'Climate', 'Energy', 'Labour',\n",
       "          'forestation', 'Forest', 'Australia', 'Environmental',\n",
       "          'industrial', 'Network', 'Battery', 'forest', 'Water', 'Animal',\n",
       "          'Closure', 'Unity', 'China', 'Nature']], dtype='<U128')],\n",
       " ['carbon',\n",
       "  array([['carbon', 'Carbon', 'Volume', 'energy', 'Water', 'Energy',\n",
       "          'liquid', 'Solar', 'Temperature', 'Environmental', 'water',\n",
       "          'Climate', 'climate', 'income', 'electricity', 'Ġcarbon',\n",
       "          'Battery', 'ĠWater', 'Delta', 'capital']], dtype='<U128')],\n",
       " ['credit',\n",
       "  array([['credit', 'Volume', 'Asset', 'Proxy', 'Agent', 'Water', 'Credit',\n",
       "          'Energy', 'Environmental', 'Legal', 'Climate', 'Solar', 'energy',\n",
       "          'liquid', 'capital', 'Ġrebate', 'Temperature', 'Power',\n",
       "          'electricity', 'income']], dtype='<U128')],\n",
       " ['capture',\n",
       "  array([['Solar', 'energy', 'Capture', 'liquid', 'storage', 'Battery',\n",
       "          'Volume', 'carbon', 'Energy', 'water', 'Storage', 'pressure',\n",
       "          'Steam', 'chemical', 'Water', 'stream', 'steam', 'electricity',\n",
       "          'Closure', 'Carbon']], dtype='<U128')],\n",
       " ['sequestration',\n",
       "  array([['storage', 'energy', 'carbon', 'Solar', 'Closure', 'Storage',\n",
       "          'Capture', 'liquid', 'water', 'Ġsequest', 'Volume', 'chemical',\n",
       "          'Carbon', 'Energy', 'Climate', 'Temperature', 'Water', 'stock',\n",
       "          'climate', 'Proxy']], dtype='<U128')],\n",
       " ['storage',\n",
       "  array([['Storage', 'storage', 'Solar', 'energy', 'Battery', 'Energy',\n",
       "          'Volume', 'liquid', 'Water', 'electricity', 'Network', 'Power',\n",
       "          'water', 'Proxy', 'carbon', 'stream', 'ĠPower', 'chemical',\n",
       "          'Climate', 'Sensor']], dtype='<U128')],\n",
       " ['Hydrogen',\n",
       "  array([['Solar', 'energy', 'Energy', 'Water', 'Battery', 'Volume',\n",
       "          'Electric', 'liquid', 'Steel', 'Power', 'Climate', 'ĠMobility',\n",
       "          'Carbon', 'Motion', 'Tesla', 'carbon', 'Science', 'Europe',\n",
       "          'Australia', 'rogen']], dtype='<U128')],\n",
       " ['Geothermal',\n",
       "  array([['Solar', 'Energy', 'energy', 'Water', 'Battery', 'Steam',\n",
       "          'Steel', 'Ocean', 'Environmental', 'Australia', 'Electric',\n",
       "          'Physical', 'Volume', 'Temperature', 'Static', 'Metal', 'Power',\n",
       "          'ĠMountain', 'liquid', 'Motion']], dtype='<U128')],\n",
       " ['Biomass',\n",
       "  array([['energy', 'Solar', 'Volume', 'Energy', 'Battery', 'Water',\n",
       "          'Animal', 'liquid', 'plant', 'Power', 'Carbon', 'Temperature',\n",
       "          'Forest', 'Environmental', 'Strength', 'animal', 'carbon',\n",
       "          'chemical', 'ĠFuel', 'ĠPower']], dtype='<U128')],\n",
       " ['Renewable',\n",
       "  array([['Solar', 'energy', 'Energy', 'Battery', 'Power', 'Electric',\n",
       "          'Australia', 'Steel', 'Water', 'Europe', 'ĠPower', 'ĠRenew',\n",
       "          'electricity', 'Pacific', 'Climate', 'ĠMobility', 'Volume',\n",
       "          'Primary', 'Atlantic', 'Canada']], dtype='<U128')],\n",
       " ['Energy',\n",
       "  array([['Energy', 'energy', 'Power', 'Europe', 'Steel', 'Australia',\n",
       "          'ĠPower', 'Solar', 'Water', 'Canada', 'Volume', 'electricity',\n",
       "          'California', 'BlackRock', 'Spain', 'Battery', 'ĠMountain',\n",
       "          'ĠWater', 'Policy', 'Environmental']], dtype='<U128')],\n",
       " ['emissions',\n",
       "  array([['emissions', 'energy', 'emission', 'Volume', 'Energy',\n",
       "          'Environmental', 'electricity', 'carbon', 'Water', 'water',\n",
       "          'income', 'Temperature', 'Ġpollution', 'output', 'chemical',\n",
       "          'product', 'issions', 'Carbon', 'climate', 'Ġwater']],\n",
       "        dtype='<U128')],\n",
       " ['reforestation',\n",
       "  array([['forestation', 'energy', 'Solar', 'forest', 'plant', 'climate',\n",
       "          'Climate', 'Water', 'Ġdrainage', 'Australia', 'Forest',\n",
       "          'ĠForest', 'Closure', 'water', 'industrial', 'Ġforest',\n",
       "          'Ġpavement', 'chemical', 'electricity', 'Environmental']],\n",
       "        dtype='<U128')],\n",
       " ['Decreased',\n",
       "  array([['Volume', 'Decre', 'Temperature', 'Increases', 'Parameter',\n",
       "          'Increased', 'Environmental', 'income', 'Policy', 'Lower',\n",
       "          'Enhanced', 'Primary', 'Climate', 'Population', 'Water',\n",
       "          'Economic', 'ĠPercentage', 'Marginal', 'Analysis', 'Physical']],\n",
       "        dtype='<U128')],\n",
       " ['Reduced',\n",
       "  array([['Volume', 'Temperature', 'Water', 'Delta', 'Primary', 'Lower',\n",
       "          'Enhanced', 'Solar', 'Energy', 'ĠWater', 'Physical', 'Australia',\n",
       "          'Climate', 'ĠPower', 'ĠAtmospheric', 'Marginal', 'Cooldown',\n",
       "          'lower', 'electricity', 'Proxy']], dtype='<U128')]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503ab0d",
   "metadata": {},
   "source": [
    "# Finding Mean of the keyword vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f7de4da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Solar', 'energy', 'Energy', 'Volume', 'Water', 'Battery',\n",
       "        'Environmental', 'Power', 'Australia', 'electricity',\n",
       "        'Temperature', 'Climate', 'liquid', 'ĠPower', 'Network',\n",
       "        'chemical', 'Electric', 'ĠWater', 'Proxy', 'Europe']],\n",
       "      dtype='<U128')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_mean = torch.stack(target, dim = 0).mean(dim = 0)\n",
    "top_20 = indexer.query(kw_mean.reshape(1, -1),return_distance = False, k = 20)\n",
    "np.array(vocab)[top_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d414211c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f56123",
   "metadata": {},
   "source": [
    "# Running BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b8b52721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5db4af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0a1ce80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True,\n",
    "                                  )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1164dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(tokenizer.vocab.keys())\n",
    "vocab = list(filter(lambda x: len(x)>4, vocab))\n",
    "#also removing tokens with '[' in the begining \n",
    "vocab = list(filter(lambda x: x[0] != '[', vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "76bb8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for ids,sen in enumerate(vocab):\n",
    "\n",
    "#     marked_text = \"[CLS] \" + sen + \" [SEP]\"\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = []\n",
    "    tokenized_text.append(\"[CLS] \")\n",
    "    tokenized_text.append(sen)\n",
    "    tokenized_text.append(\" [SEP]\")\n",
    "\n",
    "    \n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    #creating segment ids for the sentence\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "\n",
    "        hidden_states = outputs[-1]\n",
    "\n",
    "\n",
    "    #print('Tensor shape for each layer: ', hidden_states[0].size())\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # token_embeddings.size()\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat.append(sum_vec)\n",
    "\n",
    "    target.append(torch.stack(token_vecs_cat, dim = 0).mean(dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "362497a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.stack(target)\n",
    "all_embeddings = a\n",
    "normed_embeddings = (all_embeddings.T / (all_embeddings**2).sum(axis=1) ** 0.5).T\n",
    "indexer = KDTree(normed_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2baef99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['Electric vehicle',\n",
    "'Solar',\n",
    "'Wind',\n",
    "'Hydroelectric' ,\n",
    "'Nuclear',\n",
    "'REC',\n",
    "'Efficiency',\n",
    "'Deforestation',\n",
    "'Afforestation',\n",
    "'carbon',\n",
    "'credit',\n",
    "'capture',\n",
    "'sequestration',\n",
    "'storage',\n",
    "'Hydrogen',\n",
    "'Geothermal',\n",
    "'Biomass',\n",
    "'Renewable', \n",
    "'Energy',\n",
    "'emissions',\n",
    "'reforestation',\n",
    "'Decreased', \n",
    "'Reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8b8d63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for ids,sen in enumerate(keywords):\n",
    "    \n",
    "    marked_text = \"[CLS] \" + sen + \" [SEP]\"\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "\n",
    "    #creating segment ids for the sentence\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "\n",
    "        hidden_states = outputs[-1]\n",
    "\n",
    "\n",
    "    #print('Tensor shape for each layer: ', hidden_states[0].size())\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # token_embeddings.size()\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat.append(sum_vec)\n",
    "        \n",
    "    target.append(torch.stack(token_vecs_cat, dim = 0).mean(dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f949cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for emb in zip(target,keywords):\n",
    "    top_20 = indexer.query(emb[0].reshape(1, -1),return_distance = False, k = 20)\n",
    "#     print(top_20)\n",
    "    result.append([emb[1],np.array(vocab)[top_20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "143787bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Electric vehicle',\n",
       "  array([['republic', 'electronic', 'river', '##izan', 'highway',\n",
       "          'football', '##gren', 'electronics', 'reconnaissance',\n",
       "          'automobile', 'sedan', 'international', 'psalm', 'scientific',\n",
       "          'sheriff', 'helicopter', 'deputy', 'cooperative', 'herald',\n",
       "          'railway']], dtype='<U18')],\n",
       " ['Solar',\n",
       "  array([['walkway', 'parish', '##scent', '##cor', '##ckle', '##town',\n",
       "          '##utter', 'elects', 'hollis', 'correctional', '##quin',\n",
       "          'reconnaissance', 'airspace', 'heron', 'theatrical', 'southland',\n",
       "          '##bury', '##heads', 'militia', '##hoe']], dtype='<U18')],\n",
       " ['Wind',\n",
       "  array([['highway', 'tourist', 'highways', 'football', 'creeks',\n",
       "          'honduras', 'wetland', 'river', 'swamps', 'kansas', 'rural',\n",
       "          'ravens', '##vao', 'cyclone', 'public', 'france', 'national',\n",
       "          'nearby', 'washington', 'republic']], dtype='<U18')],\n",
       " ['Hydroelectric',\n",
       "  array([['##vao', '##ologist', 'provost', 'heron', '##nham', 'hectare',\n",
       "          '##hoe', 'rovers', 'parish', 'gardener', 'constabulary',\n",
       "          'paraguay', '##horn', 'correctional', '##ading', 'sociological',\n",
       "          'dozen', '##enbach', '##auer', 'neighbouring']], dtype='<U18')],\n",
       " ['Nuclear',\n",
       "  array([['##vao', '##ologist', '##osomal', 'airspace', 'correctional',\n",
       "          '##ckle', 'coven', 'national', '##hoe', '##nham', 'ordnance',\n",
       "          'vidhan', '##neo', '##ore', '##enbach', 'cyclone', 'universite',\n",
       "          '##ogist', '##scent', 'international']], dtype='<U18')],\n",
       " ['REC',\n",
       "  array([['##ologist', '##vao', 'hectare', '##aceae', 'constabulary',\n",
       "          'botanist', 'amended', 'condemn', 'rectory', 'drury', '##ogist',\n",
       "          '##sal', '##ntine', 'tourist', 'universite', 'league',\n",
       "          'hardcover', '##aud', 'incumbent', 'breuning']], dtype='<U18')],\n",
       " ['Efficiency',\n",
       "  array([['airspace', '##ice', 'walkway', '##izan', 'danube', '##ologist',\n",
       "          '##neo', '##ght', 'highway', 'kilometres', 'occupies',\n",
       "          'copeland', 'elects', '##scent', 'operative', 'clutching',\n",
       "          '##tana', '##utter', 'minute', '##oides']], dtype='<U18')],\n",
       " ['Deforestation',\n",
       "  array([['mindanao', '##orestation', 'erosion', 'saskatchewan',\n",
       "          'southwestern', 'moderate', 'globalization', 'congo', 'peoples',\n",
       "          'hectares', 'louisville', 'looting', 'honduras', 'coast',\n",
       "          'kurdish', 'topography', 'spraying', 'ceasefire', 'bolivia',\n",
       "          'deterioration']], dtype='<U18')],\n",
       " ['Afforestation',\n",
       "  array([['bearer', 'somme', 'grave', 'parish', 'crossing', 'horatio',\n",
       "          'representative', 'deadline', 'inspect', 'monument', 'member',\n",
       "          'inspection', 'pending', 'topography', 'kathy', 'northeast',\n",
       "          'visiting', 'eleventh', 'bonnie', 'tribune']], dtype='<U18')],\n",
       " ['carbon',\n",
       "  array([['theirs', '##ice', '##tana', '##cor', 'heron', 'walkway',\n",
       "          'france', 'airspace', 'highway', '##neo', '##horn', 'public',\n",
       "          '##kee', 'national', '##scent', '##hoe', '##sca', '##nham',\n",
       "          '##utter', '##ckle']], dtype='<U18')],\n",
       " ['credit',\n",
       "  array([['##ice', '##neo', '##iss', 'walkway', 'international', '##ght',\n",
       "          '##scent', '##heads', '##ous', '##utter', 'deserve', '##tters',\n",
       "          '##ogist', '##ologist', '##ckle', '##nham', '##quin', 'france',\n",
       "          'dozen', 'residents']], dtype='<U18')],\n",
       " ['capture',\n",
       "  array([['##ght', '##ologist', 'reconnaissance', 'airspace', 'crossing',\n",
       "          '##rval', '##ods', 'portico', '##scent', '##ckle', 'occupying',\n",
       "          'heron', '##heads', 'plaintiff', 'deserve', 'defendant', '##cor',\n",
       "          'defendants', 'indictment', '##tness']], dtype='<U18')],\n",
       " ['sequestration',\n",
       "  array([['democratic', 'france', 'republican', '##adt', 'republic',\n",
       "          'cinder', 'guinea', 'spain', 'zionist', '##nde', '##vao',\n",
       "          'narcotics', 'southwestern', '##izan', 'liaison', 'omnibus',\n",
       "          'north', 'international', 'south', 'daylight']], dtype='<U18')],\n",
       " ['storage',\n",
       "  array([['france', '##vao', 'schools', 'highway', 'hectare', 'tourist',\n",
       "          'kansas', 'airspace', 'correctional', 'river', 'agricultural',\n",
       "          '##tana', 'pasture', 'swamps', 'hectares', 'wetland', 'highways',\n",
       "          'defendant', 'mindanao', '##izan']], dtype='<U18')],\n",
       " ['Hydrogen',\n",
       "  array([['theirs', '##ice', '##scent', '##nham', 'highway', '##ore',\n",
       "          '##rval', '##enbach', 'airspace', '##tters', '##ckle', '##cor',\n",
       "          '##izan', '##ologist', '##rand', '##neo', 'kansas', '##utter',\n",
       "          'danube', 'rovers']], dtype='<U18')],\n",
       " ['Geothermal',\n",
       "  array([['geographical', 'geological', 'uruguayan', 'arizona', 'mindanao',\n",
       "          'tourist', 'provost', 'philology', 'archaeological', 'cambrian',\n",
       "          'topography', 'nepal', 'global', 'local', 'regional', 'congo',\n",
       "          'botanic', 'national', 'groundwater', 'domesday']], dtype='<U18')],\n",
       " ['Biomass',\n",
       "  array([['##vao', 'hectare', 'paraguay', 'hectares', 'france', 'heron',\n",
       "          'tourist', 'international', 'constabulary', 'pasture',\n",
       "          'mindanao', 'public', 'santiago', 'botanist', 'geological',\n",
       "          '##ologist', 'plaintiff', 'botany', 'kansas', 'wetland']],\n",
       "        dtype='<U18')],\n",
       " ['Renewable',\n",
       "  array([['drury', '##vao', 'correctional', 'southland', '##hoe',\n",
       "          'walkway', 'heron', 'albany', 'elects', 'parochial', '##gate',\n",
       "          'dozen', 'danube', 'hollis', '##ore', 'pasture', 'perpetual',\n",
       "          '##ice', '##urn', 'covent']], dtype='<U18')],\n",
       " ['Energy',\n",
       "  array([['##ods', '##utter', '##ice', 'walkway', '##ght', '##quin',\n",
       "          '##ckle', '##scent', 'hollis', '##tters', 'highway', '##burn',\n",
       "          '##rval', '##ord', '##heads', '##yne', '##udence', 'elects',\n",
       "          '##ore', '##omp']], dtype='<U18')],\n",
       " ['emissions',\n",
       "  array([['paraguay', 'france', 'danube', 'cinder', 'highway', 'hectares',\n",
       "          'algerian', 'strasbourg', 'wetland', 'international',\n",
       "          'kurdistan', 'hectare', 'algiers', 'creeks', 'sociological',\n",
       "          'obscene', '##ice', 'dozen', 'paving', 'schools']], dtype='<U18')],\n",
       " ['reforestation',\n",
       "  array([['tourist', '##orestation', 'mindanao', 'wetland', 'swamps',\n",
       "          'shoreline', 'highway', 'hectares', 'grassland', 'riverside',\n",
       "          'ecosystem', 'marsh', 'paving', 'newscast', 'mangrove', 'humans',\n",
       "          'ceasefire', 'spraying', 'river', 'congo']], dtype='<U18')],\n",
       " ['Decreased',\n",
       "  array([['##ologist', 'males', '##oides', '##scent', 'sentenced', '##ght',\n",
       "          'defendants', '##ous', 'defendant', 'fulbright', '##vao',\n",
       "          '##omp', '##ctuated', '##quent', '##neo', 'airspace', '##cor',\n",
       "          '##tness', '##ods', '##cea']], dtype='<U18')],\n",
       " ['Reduced',\n",
       "  array([['##ologist', 'occupying', '##ced', 'ordered', 'apparently',\n",
       "          'france', '##tness', 'crossed', 'heron', 'defendant', 'stricken',\n",
       "          'convicted', 'suspended', '##vao', 'sentenced', 'occupied',\n",
       "          '##ice', '##ght', 'public', 'condemned']], dtype='<U18')]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d261c9",
   "metadata": {},
   "source": [
    "# Finding Mean of the keyword vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f4241b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['##vao', 'highway', 'france', 'tourist', 'heron', 'public',\n",
       "        '##gren', 'football', 'correctional', '##ologist',\n",
       "        'international', 'danube', 'provost', 'hectares', '##lana',\n",
       "        '##neo', '##izan', 'airspace', 'paraguay', 'constabulary']],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_mean = torch.stack(target, dim = 0).mean(dim = 0)\n",
    "top_20 = indexer.query(kw_mean.reshape(1, -1),return_distance = False, k = 20)\n",
    "np.array(vocab)[top_20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2cd815",
   "metadata": {},
   "source": [
    "# Finding contextualized word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ccd31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [['efficiency','Our investments in efficiency helped us achieve a 22% reduction in the carbon dioxide emitted for each dollar of revenue we earned, compared to 2019.'], \n",
    "          ['energy','In 2021, we increased the amount of renewable energy in our purchased electricity to 79% compared to 41% in 2020 '],\n",
    " ['reduction','Carbon emissions from onsite combustion of fuel and purchased energy in 2021 decreased by 88,000 metric tons (MT) from 2020 .This represents a 36% reduction from the previous year and   was primarily achieved through the increased procurement of renewable electricity, as discussed above, for our North American facilities .']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9dc768e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for ids,sen in enumerate(corpus):\n",
    "    \n",
    "    marked_text = \"[CLS] \" + sen[1] + \" [SEP]\"\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "\n",
    "    #creating segment ids for the sentence\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "\n",
    "        hidden_states = outputs[-1]\n",
    "\n",
    "\n",
    "    #print('Tensor shape for each layer: ', hidden_states[0].size())\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # token_embeddings.size()\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    token_vecs_cat = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Concatenate the vectors (that is, append them together) from the last \n",
    "        # four layers.\n",
    "        # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `cat_vec` to represent `token`.\n",
    "        token_vecs_cat.append(sum_vec)\n",
    "        \n",
    "    word_index = tokenized_text.index(sen[0])\n",
    "    # Get the embedding for keyword\n",
    "    word_embedding = token_vecs_cat[word_index]\n",
    "    target.append(word_embedding / ((word_embedding**2).sum() ** 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8f988955",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for emb in zip(target,corpus):\n",
    "    top_20 = indexer.query(emb[0].reshape(1, -1),return_distance = False, k = 20)\n",
    "#     print(top_20)\n",
    "    result.append([emb[1],np.array(vocab)[top_20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "abd6e918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['efficiency',\n",
       "   'Our investments in efficiency helped us achieve a 22% reduction in the carbon dioxide emitted for each dollar of revenue we earned, compared to 2019.'],\n",
       "  array([['metabolism', 'electricity', 'schools', 'leasing',\n",
       "          'sociological', 'waterway', 'danube', 'holland', 'kurdistan',\n",
       "          'inter', 'ville', 'infrastructure', 'washington', 'ligue',\n",
       "          'freshly', 'sorbonne', 'manchester', 'terre', 'linking',\n",
       "          'polynomial']], dtype='<U18')],\n",
       " [['energy',\n",
       "   'In 2021, we increased the amount of renewable energy in our purchased electricity to 79% compared to 41% in 2020 '],\n",
       "  array([['electricity', 'spectral', 'economic', 'methane', 'thermal',\n",
       "          'petroleum', 'infrastructure', 'blazed', 'jalan', 'torch',\n",
       "          'stellar', 'going', 'spaceship', 'economically', 'oliver',\n",
       "          'transportation', 'getting', 'spice', 'ordnance', 'field']],\n",
       "        dtype='<U18')],\n",
       " [['reduction',\n",
       "   'Carbon emissions from onsite combustion of fuel and purchased energy in 2021 decreased by 88,000 metric tons (MT) from 2020 .This represents a 36% reduction from the previous year and   was primarily achieved through the increased procurement of renewable electricity, as discussed above, for our North American facilities .'],\n",
       "  array([['calculated', 'increase', 'blood', 'emission', 'showing',\n",
       "          'extinction', 'scene', 'profit', 'funded', 'import', 'flashing',\n",
       "          'decrease', 'paying', 'safety', 'voltage', 'heritage', 'towards',\n",
       "          'deviation', 'pollution', 'export']], dtype='<U18')]]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037e2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
